{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Exam <font color='red'>PART B</font> | ITSS 4382 Applied AI/ Machine Learning\n",
    "\n",
    "## <font color='red'>Open Notes, Open Internet</font>\n",
    "## <font color='red'>However, the use of ChatGPT or any external language model for directly completing or answering questions is strictly prohibited and will result in a grade of </font>ZERO<font color='red'>. Students should understand the consequences of violating this policy and the expectation of independent work during the test. Well done!</font><br><br>\n",
    "\n",
    "\n",
    "\n",
    "<font color='#ccc'>**PART A:** Answer the MCQ Q1-Q20.</font><br>\n",
    "<font color='red'>**PART B:** Answer the following questions and write your own code/answers in the boxes below.</font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the exam, you need to \n",
    " - write down your codes, printed results, and corresponding discussions in the cells below the questions.\n",
    " - add extra cells if needed using the \"+\" button or shortcut \"esc + b\", \"alt + enter\". You do NOT need to remove these cells upon submission.\n",
    " - submit your compiled .ipynb file to eLearning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_NAME = \"Winston Shih\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>PART B1. Gender Recognition by Speech Analysis</font> \n",
    "You need to include both codes and discussions in this coding file.\n",
    "\n",
    "\n",
    "##### Setting and Data\n",
    "This dataset is created to identify a voice as male or female, based upon acoustic properties of the voice and speech. It consists of 3,168 recorded voice samples, collected from male and female speakers. The voice samples are pre-processed by acoustic analysis, with an analyzed frequency range of 0Hz-280Hz (human vocal range).\n",
    "\n",
    "The CSV file contains 20 acoustic properties of each voice, and one outcome variable, “label”, which identifies the gender of the speaker. The detailed information is listed below (you do NOT need to read through the variable description). \n",
    "\n",
    "- meanfreq: mean frequency (in kHz)\n",
    "- sd: standard deviation of frequency\n",
    "- median: median frequency (in kHz)\n",
    "- Q25: first quantile (in kHz)\n",
    "- Q75: third quantile (in kHz)\n",
    "- IQR: interquantile range (in kHz)\n",
    "- skew: skewness (see note in specprop description)\n",
    "- kurt: kurtosis (see note in specprop description)\n",
    "- sp.ent: spectral entropy\n",
    "- sfm: spectral flatness\n",
    "- mode: mode frequency\n",
    "- centroid: frequency centroid (see specprop)\n",
    "- meanfun: average of fundamental frequency measured across acoustic signal\n",
    "- minfun: minimum fundamental frequency measured across acoustic signal\n",
    "- maxfun: maximum fundamental frequency measured across acoustic signal\n",
    "- meandom: average of dominant frequency measured across acoustic signal\n",
    "- mindom: minimum of dominant frequency measured across acoustic signal\n",
    "- maxdom: maximum of dominant frequency measured across acoustic signal\n",
    "- dfrange: range of dominant frequency measured across acoustic signal\n",
    "- modindx: modulation index. Calculated as the accumulated absolute difference between adjacent measurements of fundamental frequencies divided by the frequency range\n",
    "- label: male or female\n",
    "\n",
    "## Preliminaries\n",
    "Use the code below to load data and check the variable names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.180907</td>\n",
       "      <td>0.057126</td>\n",
       "      <td>0.185621</td>\n",
       "      <td>0.140456</td>\n",
       "      <td>0.224765</td>\n",
       "      <td>0.084309</td>\n",
       "      <td>3.140168</td>\n",
       "      <td>36.568461</td>\n",
       "      <td>0.895127</td>\n",
       "      <td>0.408216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180907</td>\n",
       "      <td>0.142807</td>\n",
       "      <td>0.036802</td>\n",
       "      <td>0.258842</td>\n",
       "      <td>0.829211</td>\n",
       "      <td>0.052647</td>\n",
       "      <td>5.047277</td>\n",
       "      <td>4.994630</td>\n",
       "      <td>0.173752</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.029918</td>\n",
       "      <td>0.016652</td>\n",
       "      <td>0.036360</td>\n",
       "      <td>0.048680</td>\n",
       "      <td>0.023639</td>\n",
       "      <td>0.042783</td>\n",
       "      <td>4.240529</td>\n",
       "      <td>134.928661</td>\n",
       "      <td>0.044980</td>\n",
       "      <td>0.177521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029918</td>\n",
       "      <td>0.032304</td>\n",
       "      <td>0.019220</td>\n",
       "      <td>0.030077</td>\n",
       "      <td>0.525205</td>\n",
       "      <td>0.063299</td>\n",
       "      <td>3.521157</td>\n",
       "      <td>3.520039</td>\n",
       "      <td>0.119454</td>\n",
       "      <td>0.500079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.039363</td>\n",
       "      <td>0.018363</td>\n",
       "      <td>0.010975</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.042946</td>\n",
       "      <td>0.014558</td>\n",
       "      <td>0.141735</td>\n",
       "      <td>2.068455</td>\n",
       "      <td>0.738651</td>\n",
       "      <td>0.036876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039363</td>\n",
       "      <td>0.055565</td>\n",
       "      <td>0.009775</td>\n",
       "      <td>0.103093</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.163662</td>\n",
       "      <td>0.041954</td>\n",
       "      <td>0.169593</td>\n",
       "      <td>0.111087</td>\n",
       "      <td>0.208747</td>\n",
       "      <td>0.042560</td>\n",
       "      <td>1.649569</td>\n",
       "      <td>5.669547</td>\n",
       "      <td>0.861811</td>\n",
       "      <td>0.258041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163662</td>\n",
       "      <td>0.116998</td>\n",
       "      <td>0.018223</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>0.419828</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>2.070312</td>\n",
       "      <td>2.044922</td>\n",
       "      <td>0.099766</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.184838</td>\n",
       "      <td>0.059155</td>\n",
       "      <td>0.190032</td>\n",
       "      <td>0.140286</td>\n",
       "      <td>0.225684</td>\n",
       "      <td>0.094280</td>\n",
       "      <td>2.197101</td>\n",
       "      <td>8.318463</td>\n",
       "      <td>0.901767</td>\n",
       "      <td>0.396335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184838</td>\n",
       "      <td>0.140519</td>\n",
       "      <td>0.046110</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.765795</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>4.992188</td>\n",
       "      <td>4.945312</td>\n",
       "      <td>0.139357</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.199146</td>\n",
       "      <td>0.067020</td>\n",
       "      <td>0.210618</td>\n",
       "      <td>0.175939</td>\n",
       "      <td>0.243660</td>\n",
       "      <td>0.114175</td>\n",
       "      <td>2.931694</td>\n",
       "      <td>13.648905</td>\n",
       "      <td>0.928713</td>\n",
       "      <td>0.533676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199146</td>\n",
       "      <td>0.169581</td>\n",
       "      <td>0.047904</td>\n",
       "      <td>0.277457</td>\n",
       "      <td>1.177166</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>7.007812</td>\n",
       "      <td>6.992188</td>\n",
       "      <td>0.209183</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.251124</td>\n",
       "      <td>0.115273</td>\n",
       "      <td>0.261224</td>\n",
       "      <td>0.247347</td>\n",
       "      <td>0.273469</td>\n",
       "      <td>0.252225</td>\n",
       "      <td>34.725453</td>\n",
       "      <td>1309.612887</td>\n",
       "      <td>0.981997</td>\n",
       "      <td>0.842936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251124</td>\n",
       "      <td>0.237636</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>0.279114</td>\n",
       "      <td>2.957682</td>\n",
       "      <td>0.458984</td>\n",
       "      <td>21.867188</td>\n",
       "      <td>21.843750</td>\n",
       "      <td>0.932374</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          meanfreq           sd       median          Q25          Q75  \\\n",
       "count  3168.000000  3168.000000  3168.000000  3168.000000  3168.000000   \n",
       "mean      0.180907     0.057126     0.185621     0.140456     0.224765   \n",
       "std       0.029918     0.016652     0.036360     0.048680     0.023639   \n",
       "min       0.039363     0.018363     0.010975     0.000229     0.042946   \n",
       "25%       0.163662     0.041954     0.169593     0.111087     0.208747   \n",
       "50%       0.184838     0.059155     0.190032     0.140286     0.225684   \n",
       "75%       0.199146     0.067020     0.210618     0.175939     0.243660   \n",
       "max       0.251124     0.115273     0.261224     0.247347     0.273469   \n",
       "\n",
       "               IQR         skew         kurt       sp.ent          sfm  ...  \\\n",
       "count  3168.000000  3168.000000  3168.000000  3168.000000  3168.000000  ...   \n",
       "mean      0.084309     3.140168    36.568461     0.895127     0.408216  ...   \n",
       "std       0.042783     4.240529   134.928661     0.044980     0.177521  ...   \n",
       "min       0.014558     0.141735     2.068455     0.738651     0.036876  ...   \n",
       "25%       0.042560     1.649569     5.669547     0.861811     0.258041  ...   \n",
       "50%       0.094280     2.197101     8.318463     0.901767     0.396335  ...   \n",
       "75%       0.114175     2.931694    13.648905     0.928713     0.533676  ...   \n",
       "max       0.252225    34.725453  1309.612887     0.981997     0.842936  ...   \n",
       "\n",
       "          centroid      meanfun       minfun       maxfun      meandom  \\\n",
       "count  3168.000000  3168.000000  3168.000000  3168.000000  3168.000000   \n",
       "mean      0.180907     0.142807     0.036802     0.258842     0.829211   \n",
       "std       0.029918     0.032304     0.019220     0.030077     0.525205   \n",
       "min       0.039363     0.055565     0.009775     0.103093     0.007812   \n",
       "25%       0.163662     0.116998     0.018223     0.253968     0.419828   \n",
       "50%       0.184838     0.140519     0.046110     0.271186     0.765795   \n",
       "75%       0.199146     0.169581     0.047904     0.277457     1.177166   \n",
       "max       0.251124     0.237636     0.204082     0.279114     2.957682   \n",
       "\n",
       "            mindom       maxdom      dfrange      modindx        label  \n",
       "count  3168.000000  3168.000000  3168.000000  3168.000000  3168.000000  \n",
       "mean      0.052647     5.047277     4.994630     0.173752     0.500000  \n",
       "std       0.063299     3.521157     3.520039     0.119454     0.500079  \n",
       "min       0.004883     0.007812     0.000000     0.000000     0.000000  \n",
       "25%       0.007812     2.070312     2.044922     0.099766     0.000000  \n",
       "50%       0.023438     4.992188     4.945312     0.139357     0.500000  \n",
       "75%       0.070312     7.007812     6.992188     0.209183     1.000000  \n",
       "max       0.458984    21.867188    21.843750     0.932374     1.000000  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "voice = pd.read_csv('voice.csv') \n",
    "voice['label'] = voice['label'].astype('category').cat.codes\n",
    "voice.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to use all other variables to predict the gender of the speaker (label). To start, we split the data using the  following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = voice.iloc[:, 0:19]\n",
    "y = voice.iloc[:, 20]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Scaling (5 Pts)\n",
    "\n",
    "Many machine learning methods require data scaling. In this problem, we scale our data first. \n",
    "- Use standard scaler to scale the data properly, so that the data can be applied for **supervised learning models**.\n",
    "- Note: Use the scaled data for all classification models (i.e., Q2 – Q5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2376, 19), (2376,), (792, 19), (792,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Simple Models - Naïve Bayes and Decision Tree (8 pts)\n",
    "\n",
    "- Train a Decision Tree classifier. Let max_depth be 2, and min_samples_split be 10. Set random_state to 0. What is the training accuracy? What is the test accuracy? (4 pts)\n",
    "\n",
    "- Train a Gaussian Naïve Bayes classifier. Leave all specifications as default. What is the training accuracy? What is the test accuracy? (4 pts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9532828282828283, 0.9553872053872053)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt=DecisionTreeClassifier(random_state=0, max_depth=2, min_samples_split=10)\n",
    "dt.fit(X_train, y_train)\n",
    "dt.score(X_test, y_test), dt.score(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training accuracy is 0.9553872053872053. Test accuracy is 0.9532828282828283. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "g_nb=GaussianNB()\n",
    "g_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8876262626262627, 0.8901515151515151)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_nb.score(X_test, y_test), g_nb.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training accuracy is 0.8901515151515151. Testing accuracy is 0.8876262626262627.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Logistic Regression (10 pts)\n",
    "\n",
    "- Train a logistic regression classifier and L2 regularization. Let random state be 0 and alpha be 1. What is the training accuracy? What is the test accuracy? (4 pts)\n",
    "- Report coefficient and confusion_matrix of the above model. (4 pts)\n",
    "- Suppose we want to train another logistic regression classifier with alpha = 100. Without actually training the new model, do you expect its training accuracy to be higher or lower than the previous logistic regression model (where alpha = 1)? Explain briefly.(2 pts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 0.97\n",
      "Test Set Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(penalty='l2', random_state=0, C=1)\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "y_pred = logreg.predict(X_test_scaled)\n",
    "print(f\"Training Set Accuracy: {logreg.score(X_train_scaled, y_train):.2f}\")\n",
    "print(f\"Test Set Accuracy: {logreg.score(X_test_scaled, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coef is:  [[-6.45155911e-01  1.51757724e+00 -8.01840781e-01 -5.72706493e+00\n",
      "   2.73454822e+00  8.46161316e+00 -3.69420786e-01  1.06647414e-02\n",
      "   4.04723967e+00 -1.07145506e+00  1.04883545e+00 -6.45155911e-01\n",
      "  -1.41493862e+01 -5.37550667e-02 -2.42743999e+00 -2.07347799e-01\n",
      "  -1.56904830e+00 -7.94611496e-01  7.74436803e-01]]\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "print(\"coef is: \", logreg.coef_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP is: 414\n",
      "TN is: 359\n",
      "FP is: 14\n",
      "FN is: 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "my_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"TP is:\", my_matrix[1,1])\n",
    "print(\"TN is:\", my_matrix[0,0])\n",
    "print(\"FP is:\", my_matrix[0,1])\n",
    "print(\"FN is:\", my_matrix[1,0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training accuracy would be lower if alpha increased to 100 because if alpha is increased too high, then the model \n",
    "#could underfit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Ensemble Method I (10 pts) \n",
    "\n",
    "- Train a Random Forest model. Let n_estimators be 200, max_depth be 2, and min_samples_split be 10. Set random_state to 0. What is the training accuracy? What is the test accuracy? (4 pts)\n",
    "- Train an Adaboost classifier, where each single model is a decision tree with max_depth = 2, min_samples_split = 10, and random state = 0. let n_estimators be 200. What is the training accuracy? What is the test accuracy? (4 pts)\n",
    "- Briefly discuss the similarities and differences between the two models we trained (i.e., Random Forest and Adaboost with decision tree as base model). (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy is: \n",
      "0.9945286195286195\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf=RandomForestClassifier(n_estimators=200, min_samples_split=10, \n",
    "                               random_state=0)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "print('Training accuracy is: ')\n",
    "print(rnd_clf.score(X_train, y_train))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy is: \n",
      "0.9797979797979798\n"
     ]
    }
   ],
   "source": [
    "print('Testing accuracy is: ')\n",
    "print(rnd_clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy is: \n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "naive_dt=DecisionTreeClassifier(max_depth=2, min_samples_split=10)\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    naive_dt, n_estimators=200, \n",
    "    random_state=0)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "print('Train accuracy is: ')\n",
    "print(ada_clf.score(X_train, y_train))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is: \n",
      "0.9848484848484849\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy is: ')\n",
    "print(ada_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The similarities between RandomForest and Ada Classifier are that they are Ensemble Algorithms that are used for Classifying\n",
    "#Data. However, RandomForest is a Bagging Classifier that is used to reduce variance of weaker learners. AdaBoost is a \n",
    "#BoostStrap Classifier that is supposed reduce bias of weak learners. RandomForest uses all DecisionTree outputs to produce a  \n",
    "#combined output that can reduce overfitting, encourage decision tree diversity, and improve accuracy. AdaBoost increases \n",
    "# accuracy by using every n_estimator trial to redistribute weight of trees, so incorrectly classified conditions are given more weight \n",
    "#than the conditions that are correctly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Ensemble Method II (8 pts)\n",
    "\n",
    "For now, let us focus on three simple models (without ensembling) we have trained: Decision Tree, Naïve Bayes, and Logistic Regression.\n",
    "- Use voting classifier (with hard voting) that includes the above-mentioned three models. (4 pts)\n",
    "- What is the training and test accuracy of the voting classifier? Is the test score of the voting classifier better than other models? Briefly explain why the voting classifier is/is not doing better. (3 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score for voting classifier is: 0.9448653198653199\n",
      "Test score for voting classifier is: 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "log_clf = LogisticRegression(random_state=0)\n",
    "dt_clf = DecisionTreeClassifier(random_state=0, criterion='entropy')\n",
    "nbg_clf = GaussianNB()\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "# define voting classifier\n",
    "voting_clf = VotingClassifier(estimators = [('lr1', log_clf), ('dt', dt_clf), ('nbg', nbg_clf)], voting ='hard')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "print(\"Training score for voting classifier is:\", voting_clf.score(X_train, y_train))\n",
    "print(\"Test score for voting classifier is:\", voting_clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The training and testing score of voting classifier is worse than the the scores of AdaBoost,RandomForest, Logistic Regression,\n",
    "#and Decision Tree Classifier. However, its testing and training scores are better than Naive Bayes. The voting classifier is \n",
    "# niot doing better because it includes the Naive Bayes model, which is less accurate than the other  models and caused accuracy\n",
    "#of overall classifier to be lower.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>PART B2: Clustering (10 pts)</font>\n",
    "\n",
    "Consider another scenario where we only collect the acoustic information and fail to collect the gender information (e.g., in an online setting, some users prefer not to disclose their gender). This case, we may consider using clustering methods to find groups of acoustic information similar to each other. \n",
    "- Apply MinMax Scaling to our data, so that the data can be applied for **unsupervised learning models.** (4 pts)\n",
    "- Hint: We do not need to split the data for unsupervised learning. Thus, the scaler should be trained on the full sample (i.e., X) rather than the training sample (i.e., X_train).\n",
    "- Train a K-Means model, set k=3, and random state = 0. (4 pts）\n",
    "- How many clusters do we have? Why? (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler1=MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "kmeans.fit(X_scaled)\n",
    "cls_predict = kmeans.predict(X_scaled)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of clusters:\", kmeans.n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are 3 clusters because it is the optimal number of clusters where the K Means' testing and training accuracy is \n",
    "#most accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
